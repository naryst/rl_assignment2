{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "<h1> Assignment #2 </h1>\n",
    "\n",
    "You have to move several cargos across the gridworld into a common desirable rectangle area. There are several cargos, you can move each of them either horizontally or vertically by one cell up or down. Size of the overall world may vary, as well as placement of the cargos and desirable area. The game ends when all the cargos are in the desirable area and do not overlap.\n",
    "\n",
    "Reward function: \n",
    "* for each cell of cargo placed in desirable area in the end of the turn, reward is $+1$\n",
    "* for each cell where cargos overlap in the end of the turn, reward is $-1$\n",
    "\n",
    "---\n",
    "\n",
    "Submit `{name}_{surname}.zip` file containing:\n",
    "* `{name}_{surname}.ipynb` with code for learning your model using your realisation of environment\n",
    "* `{name}_{surname}.py` script with `complete_task(path_to_infile, path_to_outfile)` function\n",
    "* report `{name}_{surname}.pdf`, containing information on:\n",
    "  * choice of the environment representation used for the model training\n",
    "  * way of training environment generation\n",
    "  * architecture choice for the neural network you use for the prediction"
   ],
   "metadata": {
    "id": "kr5ohicqUu8J",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Input of the script"
   ],
   "metadata": {
    "id": "LcYYDr7kcIam",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "`{infile}.txt` file with the field description. Elements of the field are separated by space. For example: \n",
    "\n",
    "```\n",
    "0 0 2 2 0\n",
    "0 r r r 0\n",
    "0 r r r 1\n",
    "0 r r r 1\n",
    "0 0 0 0 1\n",
    "```\n",
    "\n",
    "* `0` - blank space, we may move objects here\n",
    "* `r` - desirable area, we should move objects here\n",
    "* `1`, `2`, ... - actual object shapes, does not change, moved as a solid object"
   ],
   "metadata": {
    "id": "SmbAGt_ldNvo",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In your output file, you have to record lines in the following manner: `{id} {D/U/R/L}`. For the given example, possible sequence of steps is:\n",
    "```\n",
    "2 D\n",
    "1 L\n",
    "1 U\n",
    "2 L\n",
    "```\n",
    "\n",
    "Here the rewards are: \n",
    "1. +2 for 2 cells of `2`\n",
    "2. +4 for 2 cells of `2` and 2 cells of `1`\n",
    "3. +3 (= +4 - 1) for 2 cells of `2` and 2 cells of `1` and -1 overlapping cell \n",
    "4. **The end**"
   ],
   "metadata": {
    "id": "5yMWo5Q2c8W6",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Output of the script"
   ],
   "metadata": {
    "id": "9xrsmm3XiBtZ",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Sequence of actions, written in `{outfile}.txt`. Rewrite it after each 100th action, please, and after you rich the final position."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "score = []\n",
    "for episode in range(MAX_EPISODES):\n",
    "    env.reset()\n",
    "    total_rewards = []\n",
    "    for cargo in env.cargos:\n",
    "        current_state = np.array(env.goal_distance(cargo))\n",
    "        transitions = []\n",
    "        done = False\n",
    "        t_lens = []\n",
    "        for step in range(MAX_STEPS):\n",
    "            step_reward = -1\n",
    "\n",
    "            action = model.act(current_state, EPS_START)\n",
    "            prev_state = current_state\n",
    "\n",
    "            if env.check_move(cargo, actions[action]):\n",
    "                env.move_cargo(cargo, actions[action])\n",
    "                current_state = np.array(env.goal_distance(cargo))\n",
    "            else:\n",
    "                step_reward = -10\n",
    "\n",
    "            other_cargo_overlaps = env.get_cargo_overlaps()\n",
    "            desired_cargo_overlaps = env.get_cargo_overlaps_with_desired()\n",
    "            step_reward -= other_cargo_overlaps\n",
    "            step_reward += desired_cargo_overlaps\n",
    "            transitions.append((prev_state, action, step_reward))\n",
    "\n",
    "            cargo_num_cells = len(cargo)\n",
    "            desired_num_cells = len(env.desired_space)\n",
    "            if desired_cargo_overlaps == cargo_num_cells:\n",
    "                done = True\n",
    "                break\n",
    "\n",
    "            score.append(len(transitions))\n",
    "            reward_batch = torch.Tensor([r for (s, a, r) in\n",
    "                                         transitions]).flip(dims=(0,))\n",
    "\n",
    "            total_rewards.append(reward_batch.mean().item())\n",
    "\n",
    "            batch_Gvals = []\n",
    "\n",
    "            for i in range(len(transitions)):\n",
    "                new_G = torch.sum(reward_batch[i:] *\n",
    "                                  torch.Tensor([gamma ** j for j in\n",
    "                                                range(len(transitions) - i)]))\n",
    "                batch_Gvals.append(new_G)\n",
    "            expected_returns_batch = torch.FloatTensor(batch_Gvals)\n",
    "\n",
    "            expected_returns_batch /= expected_returns_batch.max()\n",
    "\n",
    "            state_batch = torch.Tensor(np.array([s for (s, a, r) in transitions]))\n",
    "            action_batch = torch.Tensor(np.array([a for (s, a, r) in transitions]))\n",
    "\n",
    "            pred_batch = model(state_batch)\n",
    "            prob_batch = pred_batch.gather(dim=1, index=action_batch\n",
    "                                           .long().view(-1, 1)).squeeze()\n",
    "\n",
    "            loss = F.mse_loss(prob_batch, expected_returns_batch)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if EPS_START > EPS_END and episode > (MAX_EPISODES / 2):\n",
    "                EPS_START *= EPS_DECAY\n",
    "\n",
    "        if episode % 20 == 0:\n",
    "            print('Cargo', cargo)\n",
    "            print(\"Episode: {}, Avg Reward: {}\".format(episode, np.mean(total_rewards)))\n",
    "            print('loss', loss.item())\n",
    "            print(reward_batch)\n",
    "            actions_taken = [actions[a] for (s, a, r) in transitions]\n",
    "            print('actions', actions_taken)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ]
}